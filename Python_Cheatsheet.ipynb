{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set max number of rows and cols to display\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check time to run in jupyter notebook\n",
    "%time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#instantiate matplotlib plot\n",
    "f, ax = plt.subplots(figsize=(6, 15))\n",
    "\n",
    "#create a seaborn horizontal bar plot\n",
    "sns.barplot(x=feature_importance.feature_importance, y=feature_importance.features, orient=\"h\")\n",
    "\n",
    "#Plotting multiple plots in one cell (but not on same graph)\n",
    "series = week_sale[\"total\"]\n",
    "pyplot.figure()\n",
    "pyplot.subplot(211) #the three digits stands for 2:# of rows, 1: # of cols, 1: the index of the plot (1st row)\n",
    "plot_acf(series, ax=pyplot.gca())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#count number of NAs in each column\n",
    "df.isnull().sum(axis=0)\n",
    "\n",
    "#check columns that have NAs\n",
    "df.columns[df.isna().any()]\n",
    "\n",
    "#check percentage of NA per column\n",
    "df.apply(lambda x: x.isna().sum()/len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#changing dtypes of columns. All else selected is \"number\"\n",
    "boolean = [\"Lpresent\",\"HDpresent\"]\n",
    "categorical = [\"areaname\", \"county\", \"state\"]\n",
    "\n",
    "hd_lowes[boolean] = hd_lowes[boolean].apply(lambda x: x.astype(\"bool\"))\n",
    "hd_lowes[categorical] = hd_lowes[categorical].apply(lambda x: x.astype(\"category\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imputing missingness by grouping\n",
    "#The following df has 3 variables: PClass, SexCode, Age\n",
    "\n",
    "imputation_dict = df.groupby([\"PClass\", \"SexCode\"]).mean().to_dict()\n",
    "imputation_dict['Age']\n",
    "\n",
    "#impute NAs with most recent last value\n",
    "df.fillna(method=\"ffill\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create mapping table to fill NAs based on another column\n",
    "mapping_table = df.groupby(\"col\").agg({\"col\":lambda x: int(x.mean())}).to_dict()\n",
    "\n",
    "#Fill NAs based on mapping table above (example from Simulmedia codingchallenge)\n",
    "df[\"col\"].fillna(df[\"col\"].map(mapping_table))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create an ordered set of distinct values\n",
    "from collections import OrderedDict \n",
    "\"\".join(OrderedDict.fromkeys(str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a grouper that groups data by weeks (W-Fri means it is a weekly grouper which ends on Fri)\n",
    "#must transform index to DatetimeIndex first\n",
    "grouper    = pd.Grouper(freq='W-FRI')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Change column to datetime\n",
    "df[\"date\"] = pd.to_datetime(df[\"date\"])\n",
    "\n",
    "#Change to datetime and get year/month\n",
    "pd.to_datetime(df['column']).year\n",
    "pd.to_datetime(df['column']).month\n",
    "\n",
    "#if column is already datetime, can use the following:\n",
    "df[\"column\"].dt.to_period(\"M\") # this will give YYYY-MM format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pivot tables: the below finds the average monthly temperature for each city \n",
    "df.pivot_table(index=pd.Grouper(freq='M',key='date'), columns='city', values='temperature')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#format='%m/%d/%y %H:%M'\n",
    "def str_to_datetime(df, features):\n",
    "    for feature in features:\n",
    "        df[feature]=pd.to_datetime(df[feature], format='%m/%d/%y %H:%M')\n",
    "    \n",
    "#format='%m/%d/%y'\n",
    "def str_to_date(df, features):\n",
    "    for feature in features:\n",
    "        df[feature]=pd.to_datetime(df[feature], format='%m/%d/%y')\n",
    "\n",
    "#format='%m/%d/%Y'\n",
    "def str_to_date1(df, features):\n",
    "    for feature in features:\n",
    "        df[feature]=pd.to_datetime(df[feature], format='%m/%d/%Y')\n",
    "\n",
    "#str to numeric\n",
    "def num(df, features):\n",
    "    for feature in features:\n",
    "        df[feature]=df[feature].replace('[$,() ]', '', regex=True).astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualize distribtion of binned age\n",
    "binned_hh_age_count = df[\"binned_head_of_household_age\"].value_counts()\n",
    "binned_second_hh_age_count = df[\"binned_second_head_of_household_age\"].value_counts()\n",
    "\n",
    "#Plot head of household age distrib\n",
    "fig,(ax0,ax1) = plt.subplots(2,1)\n",
    "fig.subplots_adjust(hspace=0.5)\n",
    "\n",
    "binned_hh_age_count.loc[labels].plot(kind=\"bar\",\n",
    "                         ax=ax0,\n",
    "                         figsize=(10,9),\n",
    "                         title=\"Distribution of head of household age\")\n",
    "ax0.set_xlabel(\"Age\")\n",
    "ax0.set_ylabel(\"Frequency\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#changing column from categorical to 1's and 0's\n",
    "df[\"column\"].apply(lambda x: 1 if x==\"Yes\" else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stratefied train/test split (splitting 80/20 split) - for categorical variable\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#here the stratify=y is referring to y, the predictor variable and not \"yes\"\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "\n",
    "#Stratefied for continuous variable, you will first have to create col with bins to make it 'categorical'\n",
    "housing[\"income_cat\"]=np.ceil(housing[\"median_income\"] / 1.5)\n",
    "housing[\"income_cat\"].where(housing[\"income_cat\"] <5, 5.0, inplace=True) #keep original data if <5, anything above bucket as 5\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(housing.drop(columns=[\"income_cat\",\"median_house_value\"], axis=1), \n",
    "                                                    housing[\"median_house_value\"],\n",
    "                                                    test_size=0.2, random_state=42, \n",
    "                                                    stratify=housing[\"income_cat\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dummify categorical variables\n",
    "use_columns = [categorical variables to use]\n",
    "\n",
    "pd.get_dummies(df[use_columns], drop_first=True, dummy_na=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gridsearch with RandomForest\n",
    "grid_para_forest = [{\n",
    "    \"n_estimators\": range(100,201,50),\n",
    "    \"n_jobs\": [-1],\n",
    "    \"max_features\": [\"auto\",\"sqrt\"],\n",
    "    \"min_samples_leaf\": [1,2,3],\n",
    "    \"random_state\": [42]\n",
    "    }]\n",
    "\n",
    "#n_jobs = -1 uses all processors in your computer to do parallel run \n",
    "grid_search_forest = GridSearchCV(randomForest, grid_para_forest, scoring='roc_auc', cv=5, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Feature Importance with visualizations\n",
    "\n",
    "xgb_FI = pd.Series(data=grid_xgb.best_estimator_.feature_importances_, \n",
    "                   index=X.columns)\n",
    "\n",
    "xgb_FI.sort_values().plot(kind=\"barh\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TfidfVectorizer is a module that assigns weights to texts \n",
    "# common words will have low weights and special words have high weights\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "model = make_pipeline(TfidfVectorizer(), MultinomialNB()\n",
    "model.fit(train_x, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Confusion matrix and heatmap\n",
    "from sklearn.metrics import confusion_matrix\n",
    "mat = confusion_matrix(test_y, predicted_y)\n",
    "sns.heatmap(mat.T, square=True, annot=True, fmt='d', cbar=False,\n",
    "           xticklabels=train_y,\n",
    "           yticklabels=train_y)\n",
    "plt.xlabel('True Labels')\n",
    "plt.ylabel(\"Predicted Labels\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Kmeans elbow plot\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "k = range(1, 10)\n",
    "km = [KMeans(n_clusters=i) for i in Ks]\n",
    "score = [km[i].fit(my_matrix).score(my_matrix) for i in range(len(km))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#packages\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.pipeline import make_pipeline, FeatureUnion, Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, Imputer, StandardScaler\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "#Dataframe Selector for preprocessing in a pipeline\n",
    "class DataFrameSelector(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, attribute_names):\n",
    "        self.attribute_names = attribute_names\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        return X[self.attribute_names]\n",
    "\n",
    "numeric_feats = train.dtypes[train.dtypes != \"object\"].index\n",
    "\n",
    "num_pipeline = Pipeline([\n",
    "    ('selector', DataFrameSelector(numeric_feats)),\n",
    "    ('imputer', SimpleImputer(strategy=\"median\")),\n",
    "    ('scalar', StandardScaler()),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Column Selector\n",
    "\n",
    "class ColumnSelector(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, columns):\n",
    "        self.columns = columns\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        assert isinstance(X, pd.DataFrame)\n",
    "        try:\n",
    "            return X[self.columns]\n",
    "        except KeyError:\n",
    "            cols_error = list(set(self.columns) - set(X.columns))\n",
    "            raise KeyError(\"The DataFrame does not include the columns: %s\" % cols_error)\n",
    "\n",
    "# Select columns you want by specifying which ones to exclude\n",
    "x_cols = [c for c in df if c not in [\"target\", \"phone number\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#build a TypeSelector for the pipeline\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "class TypeSelector(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, dtype):\n",
    "        self.dtype = dtype\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        assert isinstance(X, pd.DataFrame)\n",
    "        return X.select_dtypes(include=[self.dtype])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#build a StringIndexer to allow onehotencoder to work properly\n",
    "class StringIndexer(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        assert isinstance(X, pd.DataFrame)\n",
    "        return X.apply(lambda s: s.cat.codes.replace(\n",
    "            {-1: len(s.cat.categories)}\n",
    "        ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#build out transformer\n",
    "## IMPORTANT: only include the dtypes that the df actually has, \n",
    "## ie if X_train doesnt have bool dtype cols, it will return an error!\n",
    "\n",
    "preprocess_pipeline = Pipeline([\n",
    "    ('features', FeatureUnion(n_jobs=1, transformer_list=[\n",
    "        ('boolean', Pipeline([\n",
    "            ('selector', TypeSelector('bool')),\n",
    "            ('imputer', Imputer(strategy=\"most_frequent\")),\n",
    "        ])),  # booleans close\n",
    "        \n",
    "        ('numericals', Pipeline([\n",
    "            ('selector', TypeSelector(np.number)),\n",
    "            ('imputer', Imputer(strategy=\"median\")),\n",
    "            ('scaler', StandardScaler()),\n",
    "        ])),  # numericals close\n",
    "        \n",
    "        ('categoricals', Pipeline([\n",
    "            ('selector', TypeSelector('category')),\n",
    "            ('imputer', Imputer(strategy=\"most_frequent\")),\n",
    "            ('labeler', StringIndexer()),\n",
    "            ('encoder', OneHotEncoder(handle_unknown='ignore')),\n",
    "        ]))  # categoricals close\n",
    "    ])),  # features close\n",
    "])  # pipeline close"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "#Reading the training and testing data \n",
    "train = pd.read_csv('../data/train.csv')\n",
    "test = pd.read_csv('../data/test.csv')\n",
    "\n",
    "#dummify the dataset to handle categorical data\n",
    "train = pd.get_dummies(train, drop_first=True)\n",
    "\n",
    "#Dataframe Selector for preprocessing in a pipeline\n",
    "class DataFrameSelector(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, attribute_names):\n",
    "        self.attribute_names = attribute_names\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        return X[self.attribute_names]\n",
    "    \n",
    "#Create pipeline for numerical data\n",
    "num_features = train.dtypes[train.dtypes != \"object\"].index\n",
    "\n",
    "num_pipeline = Pipeline([\n",
    "    ('selector', DataFrameSelector(num_features)),\n",
    "    ('imputer', SimpleImputer(strategy=\"median\")),\n",
    "    ('scalar', StandardScaler()),\n",
    "])\n",
    "\n",
    "#Separating target and predictor variables\n",
    "X = train.drop([\"SalePrice\"], axis=1)\n",
    "y = train[\"SalePrice\"]\n",
    "\n",
    "#Log transformation of Saleprice\n",
    "y_log = np.log1p(y)\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y_log, test_size=0.25)\n",
    "\n",
    "X_train = num_pipeline.fit_transform(X_train)\n",
    "X_val = num_pipeline.fit_transform(X_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compare models\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from xgboost.sklearn import XGBRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score\n",
    "\n",
    "#Training RMSE:\n",
    "\n",
    "##Lasso\n",
    "lasso = Lasso()\n",
    "las_scores = cross_val_score(lasso, X_train, y_train, scoring=\"neg_mean_squared_error\", cv=5)\n",
    "print(\"Lasso Train RMSE: {}\".format(np.sqrt(-las_scores).mean())) \n",
    "\n",
    "##RandomForest\n",
    "rf = RandomForestRegressor()\n",
    "rf_scores = cross_val_score(rf, X_train, y_train, scoring=\"neg_mean_squared_error\", cv=5)\n",
    "print(\"Random Forest Train RMSE: {}\".format(np.sqrt(-rf_scores).mean()))\n",
    "\n",
    "##XGBoost\n",
    "xgb = XGBRegressor(n_jobs=-1)\n",
    "xgb_scores = cross_val_score(xgb, X_train, y_train, scoring=\"neg_mean_squared_error\", cv=5)\n",
    "print(\"XGBoost Train RMSE: {}\".format(np.sqrt(-xgb_scores).mean()))\n",
    "\n",
    "#Validation RMSE:\n",
    "\n",
    "##Lasso\n",
    "lasso.fit(X_train,y_train)\n",
    "print(\"Lasso Test RMSE: {}\" .format(np.sqrt(mean_squared_error(y_val, lasso.predict(X_val)))))\n",
    "\n",
    "##RandomForest\n",
    "rf.fit(X_train,y_train)\n",
    "print(\"RF Test RMSE: {}\" .format(np.sqrt(mean_squared_error(y_val, rf.predict(X_val)))))\n",
    "\n",
    "##XGBoost\n",
    "xgb.fit(X_train,y_train)\n",
    "print(\"XGB Test RMSE: {}\" .format(np.sqrt(mean_squared_error(y_val, xgb.predict(X_val)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Classifier Switcher\n",
    "from sklearn.base import BaseEstimator\n",
    "\n",
    "class ClfSwitcher(BaseEstimator):\n",
    "\n",
    "    def __init__(\n",
    "        self, \n",
    "        estimator = SGDClassifier(),\n",
    "    ):\n",
    "        \"\"\"\n",
    "        A Custom BaseEstimator that can switch between classifiers.\n",
    "        :param estimator: sklearn object - The classifier\n",
    "        \"\"\" \n",
    "\n",
    "        self.estimator = estimator\n",
    "\n",
    "\n",
    "    def fit(self, X, y=None, **kwargs):\n",
    "        self.estimator.fit(X, y)\n",
    "        return self\n",
    "\n",
    "\n",
    "    def predict(self, X, y=None):\n",
    "        return self.estimator.predict(X)\n",
    "\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        return self.estimator.predict_proba(X)\n",
    "\n",
    "\n",
    "    def score(self, X, y):\n",
    "        return self.estimator.score(X, y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
